{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5SV8FqkjAot"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': array([ 1.34183265e+00,  7.49100387e-01,  5.34722720e-01,  1.97805133e-04,\n",
       "         7.15193042e-05,  7.73933014e-06,  5.51992816e-08, -2.42927453e-06,\n",
       "         4.73325650e-06, -2.28455228e-06]),\n",
       " 'achieved_goal': array([1.34183265, 0.74910039, 0.53472272]),\n",
       " 'desired_goal': array([1.3752391 , 0.6333266 , 0.53754559])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import OpenAI gym\n",
    "import gym\n",
    "\n",
    "# create the environment\n",
    "env = gym.make('FetchReach-v1')\n",
    "\n",
    "# initialize the env\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZcSh0OJjAoy"
   },
   "source": [
    "### Display Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJPUpslZjAoz"
   },
   "outputs": [],
   "source": [
    "# show a few frames of CartPole\n",
    "for i in range(100):\n",
    "    # display the env (optional)\n",
    "#     env.render()\n",
    "    # randomly chose an action from all available actions\n",
    "    action = env.action_space.sample()\n",
    "    # agent takes an action and interacts with the env, receiving state, reward, done and info\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # if episode is over reset the env\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Environment Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space is Dict(achieved_goal:Box(-inf, inf, (3,), float32), desired_goal:Box(-inf, inf, (3,), float32), observation:Box(-inf, inf, (10,), float32))\n",
      "\n",
      "action space is Box(-1.0, 1.0, (4,), float32)\n",
      "\n",
      "example state is {'observation': array([ 1.34183265e+00,  7.49100387e-01,  5.34722720e-01,  1.97805133e-04,\n",
      "        7.15193042e-05,  7.73933014e-06,  5.51992816e-08, -2.42927453e-06,\n",
      "        4.73325650e-06, -2.28455228e-06]), 'achieved_goal': array([1.34183265, 0.74910039, 0.53472272]), 'desired_goal': array([1.21307256, 0.79961713, 0.65835359])}\n",
      "\n",
      "example action is [-0.9026402  -0.26175523  0.3022741  -0.46299675]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"state space is {}\\n\".format(env.observation_space))\n",
    "print(\"action space is {}\\n\".format(env.action_space))\n",
    "print(\"example state is {}\\n\".format(env.reset()))\n",
    "print(\"example action is {}\\n\".format(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6BNrKZKjAo3"
   },
   "source": [
    "### The Reinforcement Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZTUa79SjAo4",
    "outputId": "89ed6c1f-780b-46fe-b3fd-d519287508f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done with reward: 50 in 50 iterations\n",
      "Episode 1 done with reward: 50 in 50 iterations\n",
      "Episode 2 done with reward: 50 in 50 iterations\n",
      "Episode 3 done with reward: 50 in 50 iterations\n",
      "Episode 4 done with reward: 50 in 50 iterations\n",
      "Episode 5 done with reward: 50 in 50 iterations\n",
      "Episode 6 done with reward: 50 in 50 iterations\n",
      "Episode 7 done with reward: 50 in 50 iterations\n",
      "Episode 8 done with reward: 50 in 50 iterations\n",
      "Episode 9 done with reward: 50 in 50 iterations\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "# run environment for 10 episodes\n",
    "for ep in range(episodes):\n",
    "    episode_reward = 0\n",
    "    count_while = 0\n",
    "    while True:\n",
    "        # randomly chose an action from all available actions\n",
    "        action = env.action_space.sample()\n",
    "        # agent takes an action and interacts with the env, receiving state, reward, done and info\n",
    "        state, reward, done, info = env.step(action)\n",
    "        episode_reward += 1\n",
    "        count_while += 1\n",
    "        # if episode is over reset the env\n",
    "        if done:\n",
    "            print(\"Episode {} done with reward: {} in {} iterations\".format(ep, episode_reward,count_while))\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVrPNJyljAo9"
   },
   "source": [
    "### Episodes and Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lQ9hFdejAo-",
    "outputId": "c4f5db15-06c7-4025-f64c-953f5af899ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 50 timesteps\n",
      "Episode 1 done after 50 timesteps\n",
      "Episode 2 done after 50 timesteps\n",
      "Episode 3 done after 50 timesteps\n",
      "Episode 4 done after 50 timesteps\n",
      "Episode 5 done after 50 timesteps\n",
      "Episode 6 done after 50 timesteps\n",
      "Episode 7 done after 50 timesteps\n",
      "Episode 8 done after 50 timesteps\n",
      "Episode 9 done after 50 timesteps\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "max_timesteps = 200\n",
    "# run environment for 10 episodes\n",
    "for ep in range(episodes):\n",
    "    timestep = 0\n",
    "    while timestep < max_timesteps:\n",
    "        # randomly chose an action from all available actions\n",
    "        action = env.action_space.sample()\n",
    "        # agent takes an action and interacts with the env, receiving state, reward, done and info\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(reward)\n",
    "        timestep += 1\n",
    "        # if episode is over reset the env\n",
    "        if done:\n",
    "            print(\"Episode {} done after {} timesteps\".format(ep, timestep))\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSL7bwx_jApC"
   },
   "source": [
    "### Interacting with the Environment: actions, done and env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuH0Acv_jApC",
    "outputId": "fb4d2d0c-4810-4729-b788-f19d3ed37e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1: agent took action [ 0.7560593  -0.6924245  -0.24547774 -0.39658237]\n",
      "\n",
      "Timestep 1: state {'observation': array([ 1.36410827e+00,  7.27643151e-01,  5.26939023e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.95180012e-02, -1.82222498e-02, -6.66460540e-03,\n",
      "        4.39849041e-04,  3.49064332e-04]), 'achieved_goal': array([1.36410827, 0.72764315, 0.52693902]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 2: agent took action [-0.17267902  0.40358633  0.47622138  0.9253453 ]\n",
      "\n",
      "Timestep 2: state {'observation': array([ 1.36114808e+00,  7.38084262e-01,  5.40679097e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -7.01393735e-03,  1.24937954e-02,  1.31277325e-02,\n",
      "        1.42609132e-04,  1.06358565e-03]), 'achieved_goal': array([1.36114808, 0.73808426, 0.5406791 ]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 3: agent took action [ 0.9690221   0.4367408  -0.46284202  0.28184357]\n",
      "\n",
      "Timestep 3: state {'observation': array([ 1.38871878e+00,  7.52967654e-01,  5.27490698e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.57694716e-02,  1.01007438e-02, -1.40507262e-02,\n",
      "        2.41473119e-04,  6.06195946e-04]), 'achieved_goal': array([1.38871878, 0.75296765, 0.5274907 ]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 4: agent took action [-0.12516508 -0.511653   -0.0349574  -0.48200005]\n",
      "\n",
      "Timestep 4: state {'observation': array([ 1.38803095e+00,  7.38334246e-01,  5.24844403e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -6.29351391e-03, -1.43889377e-02,  6.10152130e-04,\n",
      "        8.01426478e-04,  1.92059803e-04]), 'achieved_goal': array([1.38803095, 0.73833425, 0.5248444 ]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 5: agent took action [ 0.88790977  0.9564755   0.9953499  -0.36445424]\n",
      "\n",
      "Timestep 5: state {'observation': array([1.41248414e+00, 7.66481789e-01, 5.55427334e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 2.23490572e-02, 2.68008949e-02, 2.62265016e-02,\n",
      "       3.64425909e-04, 1.18850403e-03]), 'achieved_goal': array([1.41248414, 0.76648179, 0.55542733]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 6: agent took action [-0.9795895   0.24809955 -0.6750255   0.87301844]\n",
      "\n",
      "Timestep 6: state {'observation': array([ 1.38581462e+00,  7.76806647e-01,  5.37964004e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -2.90480396e-02,  3.33484193e-03, -2.05206223e-02,\n",
      "        9.40199092e-04,  1.20767948e-04]), 'achieved_goal': array([1.38581462, 0.77680665, 0.537964  ]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 7: agent took action [ 0.02634737  0.702055    0.29707107 -0.38242432]\n",
      "\n",
      "Timestep 7: state {'observation': array([1.38300432e+00, 7.98664198e-01, 5.44707443e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 3.70747708e-03, 1.79737454e-02, 1.00980079e-02,\n",
      "       3.06809699e-04, 4.18046771e-04]), 'achieved_goal': array([1.38300432, 0.7986642 , 0.54470744]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 8: agent took action [0.9271221  0.1900561  0.34422845 0.5896803 ]\n",
      "\n",
      "Timestep 8: state {'observation': array([ 1.41020996e+00,  8.06306513e-01,  5.56126106e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  2.31412824e-02,  2.89806968e-03,  7.78837825e-03,\n",
      "        6.58126067e-04, -1.62902035e-04]), 'achieved_goal': array([1.41020996, 0.80630651, 0.55612611]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 9: agent took action [-0.05765098 -0.10617407 -0.24808277  0.84572446]\n",
      "\n",
      "Timestep 9: state {'observation': array([ 1.41134018e+00,  8.03392224e-01,  5.49366451e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -4.03602504e-03, -3.08052770e-03, -7.45541757e-03,\n",
      "        6.08208036e-04,  5.70429340e-05]), 'achieved_goal': array([1.41134018, 0.80339222, 0.54936645]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n",
      "Timestep 10: agent took action [ 0.68301386 -0.12952591 -0.04100513  0.17002493]\n",
      "\n",
      "Timestep 10: state {'observation': array([ 1.43057407e+00,  7.98942159e-01,  5.47006970e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.78623006e-02, -3.09108852e-03, -3.53642593e-04,\n",
      "        3.99086423e-04,  7.15224929e-05]), 'achieved_goal': array([1.43057407, 0.79894216, 0.54700697]), 'desired_goal': array([1.19615138, 0.73346856, 0.62849142])}, reward -1.0, done False, info {'is_success': 0.0}\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "\n",
    "max_timesteps = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    timestep = 0\n",
    "    while timestep < max_timesteps:\n",
    "        # randomly chose an action from all available actions\n",
    "        action = env.action_space.sample()\n",
    "        # agent takes an actiona nd interacts with the env, receiving state, reward, done and info\n",
    "        state, reward, done, info = env.step(action)\n",
    "        timestep += 1\n",
    "        print(\"Timestep {}: agent took action {}\\n\".format(timestep, action))\n",
    "        print(\"Timestep {}: state {}, reward {}, done {}, info {}\\n\\n-----\".format(timestep, state, reward, done, info))\n",
    "        # if episode is over reset the env\n",
    "        if done:\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4501722e+00,  6.2254441e-01,  4.4336995e-01,  1.3418326e+00,\n",
       "        7.4910039e-01,  5.3472275e-01,  1.9780513e-04,  7.1519302e-05,\n",
       "        7.7393297e-06,  5.5199283e-08, -2.4292744e-06,  4.7332564e-06,\n",
       "       -2.2845522e-06], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym.wrappers import FilterObservation, FlattenObservation\n",
    "env = gym.make('FetchReach-v1')\n",
    "# create single observation space array\n",
    "env = FlattenObservation(FilterObservation(env, ['desired_goal','observation']))\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "1.6 Understanding OpenAI Gym Environments.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
